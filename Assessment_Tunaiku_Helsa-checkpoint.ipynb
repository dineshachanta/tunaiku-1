{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. add default values to a python dictionary values, whenever there is no data in them\n",
    "example : input >> {k1:v1, k2:'blank',k3:v3,k4:''blank',k5:''blank'}\n",
    "expected output >> {k1:v1, k2:'default',k3:v3,k4:''default',k5:''default'}\n",
    "\n",
    "hint : look at the python dict methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "str_input = '''{\"k1\":\"v1\", \"k2\":\"blank\",\"k3\":\"v3\",\"k4\":\"'blank\",\"k5\":\"'blank\"}'''\n",
    "json_input = json.loads(str_input)\n",
    "json_output = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in json_input.keys():\n",
    "    if (\"blank\" in json_input[k] or json_input[k] == \"\"):\n",
    "        json_input[k] = \"default\"\n",
    "    json_output = json.dumps(json_input,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"k1\": \"v1\", \"k2\": \"default\", \"k3\": \"v3\", \"k4\": \"default\", \"k5\": \"default\"}\n"
     ]
    }
   ],
   "source": [
    "print(json_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. read a config file having flags, and create metric def based on flag values\n",
    "example config file : here\n",
    "Based on flag values, create metrics for all the demo_codes and store them in a python dict\n",
    "\n",
    "hint : use system modules and try to store the file data in a dict as a starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileconfig = open(\"D:/Learn/tunaiku/fileconfig.txt\",\"r\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "data_mode = {}\n",
    "data_final = {}\n",
    "for rows in fileconfig.read().split(\"\\n\"):\n",
    "    data_mode[\"demo_code\"] = rows.split(\"|\")[0]\n",
    "    data_mode[\"c2_5_flag\"] = rows.split(\"|\")[1]\n",
    "    data_mode[\"c6_11_flag\"] = rows.split(\"|\")[2]\n",
    "    data_mode[\"m12_14_flag\"] = rows.split(\"|\")[3]\n",
    "    data_mode[\"m15_17_flag\"] = rows.split(\"|\")[4]\n",
    "    data_mode[\"m18_20_flag\"] = rows.split(\"|\")[5]\n",
    "    data_mode[\"m21_24_flag\"] = rows.split(\"|\")[6]\n",
    "    data_mode[\"m25_34_flag\"] = rows.split(\"|\")[7]\n",
    "    data_mode[\"m35_49_flag\"] = rows.split(\"|\")[8]\n",
    "    data_mode[\"m50_54_flag\"] = rows.split(\"|\")[9]\n",
    "    data_mode[\"m55_64_flag\"] = rows.split(\"|\")[10]\n",
    "    data_mode[\"m65p_flag\"] = rows.split(\"|\")[11]\n",
    "    data_mode[\"f12_14_flag\"] = rows.split(\"|\")[12]\n",
    "    data_mode[\"f15_17_flag\"] = rows.split(\"|\")[13]\n",
    "    data_mode[\"f18_20_flag\"] = rows.split(\"|\")[14]\n",
    "    data_mode[\"f21_24_flag\"] = rows.split(\"|\")[15]\n",
    "    data_mode[\"f25_34_flag\"] = rows.split(\"|\")[16]\n",
    "    data_mode[\"f35_49_flag\"] = rows.split(\"|\")[17]\n",
    "    data_mode[\"f50_54_flag\"] = rows.split(\"|\")[18]\n",
    "    data_mode[\"f55_64_flag\"] = rows.split(\"|\")[19]\n",
    "    data_mode[\"f65p_flag\"] = rows.split(\"|\")[20]\n",
    "    data_mode[\"ww_flag\"] = rows.split(\"|\")[21]\n",
    "    json_final = json.dumps(data_mode, ensure_ascii=False)\n",
    "    data.append(json_final)\n",
    "data_final[\"data\"] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileconfig.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': ['{\"demo_code\": \"M2164\", \"c2_5_flag\": \"0\", \"c6_11_flag\": \"0\", \"m12_14_flag\": \"0\", \"m15_17_flag\": \"0\", \"m18_20_flag\": \"0\", \"m21_24_flag\": \"1\", \"m25_34_flag\": \"1\", \"m35_49_flag\": \"1\", \"m50_54_flag\": \"1\", \"m55_64_flag\": \"1\", \"m65p_flag\": \"0\", \"f12_14_flag\": \"0\", \"f15_17_flag\": \"0\", \"f18_20_flag\": \"0\", \"f21_24_flag\": \"0\", \"f25_34_flag\": \"0\", \"f35_49_flag\": \"0\", \"f50_54_flag\": \"0\", \"f55_64_flag\": \"0\", \"f65p_flag\": \"0\", \"ww_flag\": \"0\"}', '{\"demo_code\": \"P634\", \"c2_5_flag\": \"0\", \"c6_11_flag\": \"1\", \"m12_14_flag\": \"1\", \"m15_17_flag\": \"1\", \"m18_20_flag\": \"1\", \"m21_24_flag\": \"1\", \"m25_34_flag\": \"1\", \"m35_49_flag\": \"0\", \"m50_54_flag\": \"0\", \"m55_64_flag\": \"0\", \"m65p_flag\": \"0\", \"f12_14_flag\": \"1\", \"f15_17_flag\": \"1\", \"f18_20_flag\": \"1\", \"f21_24_flag\": \"1\", \"f25_34_flag\": \"1\", \"f35_49_flag\": \"0\", \"f50_54_flag\": \"0\", \"f55_64_flag\": \"0\", \"f65p_flag\": \"0\", \"ww_flag\": \"0\"}', '{\"demo_code\": \"B1520\", \"c2_5_flag\": \"0\", \"c6_11_flag\": \"0\", \"m12_14_flag\": \"0\", \"m15_17_flag\": \"1\", \"m18_20_flag\": \"1\", \"m21_24_flag\": \"0\", \"m25_34_flag\": \"0\", \"m35_49_flag\": \"0\", \"m50_54_flag\": \"0\", \"m55_64_flag\": \"0\", \"m65p_flag\": \"0\", \"f12_14_flag\": \"0\", \"f15_17_flag\": \"0\", \"f18_20_flag\": \"0\", \"f21_24_flag\": \"0\", \"f25_34_flag\": \"0\", \"f35_49_flag\": \"0\", \"f50_54_flag\": \"0\", \"f55_64_flag\": \"0\", \"f65p_flag\": \"0\", \"ww_flag\": \"0\"}', '{\"demo_code\": \"W3554\", \"c2_5_flag\": \"0\", \"c6_11_flag\": \"0\", \"m12_14_flag\": \"0\", \"m15_17_flag\": \"0\", \"m18_20_flag\": \"0\", \"m21_24_flag\": \"0\", \"m25_34_flag\": \"0\", \"m35_49_flag\": \"0\", \"m50_54_flag\": \"0\", \"m55_64_flag\": \"0\", \"m65p_flag\": \"0\", \"f12_14_flag\": \"0\", \"f15_17_flag\": \"0\", \"f18_20_flag\": \"0\", \"f21_24_flag\": \"0\", \"f25_34_flag\": \"0\", \"f35_49_flag\": \"1\", \"f50_54_flag\": \"1\", \"f55_64_flag\": \"0\", \"f65p_flag\": \"0\", \"ww_flag\": \"0\"}', '{\"demo_code\": \"W5564\", \"c2_5_flag\": \"0\", \"c6_11_flag\": \"0\", \"m12_14_flag\": \"0\", \"m15_17_flag\": \"0\", \"m18_20_flag\": \"0\", \"m21_24_flag\": \"0\", \"m25_34_flag\": \"0\", \"m35_49_flag\": \"0\", \"m50_54_flag\": \"0\", \"m55_64_flag\": \"0\", \"m65p_flag\": \"0\", \"f12_14_flag\": \"0\", \"f15_17_flag\": \"0\", \"f18_20_flag\": \"0\", \"f21_24_flag\": \"0\", \"f25_34_flag\": \"0\", \"f35_49_flag\": \"0\", \"f50_54_flag\": \"0\", \"f55_64_flag\": \"1\", \"f65p_flag\": \"0\", \"ww_flag\": \"0\"}', '{\"demo_code\": \"G1220\", \"c2_5_flag\": \"0\", \"c6_11_flag\": \"0\", \"m12_14_flag\": \"0\", \"m15_17_flag\": \"0\", \"m18_20_flag\": \"0\", \"m21_24_flag\": \"0\", \"m25_34_flag\": \"0\", \"m35_49_flag\": \"0\", \"m50_54_flag\": \"0\", \"m55_64_flag\": \"0\", \"m65p_flag\": \"0\", \"f12_14_flag\": \"1\", \"f15_17_flag\": \"1\", \"f18_20_flag\": \"1\", \"f21_24_flag\": \"0\", \"f25_34_flag\": \"0\", \"f35_49_flag\": \"0\", \"f50_54_flag\": \"0\", \"f55_64_flag\": \"0\", \"f65p_flag\": \"0\", \"ww_flag\": \"0\"}']}\n"
     ]
    }
   ],
   "source": [
    "print(data_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load a file having size > 2gb from google cloud storage to a bigquery table by python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I particularly still have not figuring out about GC Storage and bigquery , but i try to google , so here is my answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.cloud import bigquery\n",
    "# client = bigquery.Client()\n",
    "# dataset_id = 'my_dataset'\n",
    "\n",
    "dataset_ref = client.dataset(dataset_id)\n",
    "job_config = bigquery.LoadJobConfig()\n",
    "job_config.schema = [\n",
    "    bigquery.SchemaField(\"name\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"post_abbr\", \"STRING\"),\n",
    "]\n",
    "job_config.skip_leading_rows = 1\n",
    "# The source format defaults to CSV, so the line below is optional.\n",
    "job_config.source_format = bigquery.SourceFormat.CSV\n",
    "uri = \"gs://cloud-samples-data/bigquery/us-states/us-states.csv\"\n",
    "\n",
    "load_job = client.load_table_from_uri(\n",
    "    uri, dataset_ref.table(\"us_states\"), job_config=job_config\n",
    ")  # API request\n",
    "print(\"Starting job {}\".format(load_job.job_id))\n",
    "\n",
    "load_job.result()  # Waits for table load to complete.\n",
    "print(\"Job finished.\")\n",
    "\n",
    "destination_table = client.get_table(dataset_ref.table(\"us_states\"))\n",
    "print(\"Loaded {} rows.\".format(destination_table.num_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Split a time interval into equal portions\n",
    "Example input >> start_time:  2020-03-02 13:02:00 end_time : 2020-03-02 15:55:07\n",
    "Expected output :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-0ed8588a3774>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mhours\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%H:%M\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminutes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminutes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminute\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mdays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhours\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime,timedelta\n",
    "import dateutil\n",
    "\n",
    "start_time = datetime.strptime(\"2020-03-02 13:02:00\",\"%Y-%m-%d %H:%M:%S\")\n",
    "end_time = datetime.strptime(\"2020-03-02 15:55:07\",\"%Y-%m-%d %H:%M:%S\")\n",
    "hours = []\n",
    "days = []\n",
    "diff = (end_time  - start_time )\n",
    "while start_time <= end_time:\n",
    "    hours.append(start_time.strftime(\"%H:%M\"))\n",
    "    start_time += timedelta(minutes=30)-timedelta(minutes=(start_time.minute))\n",
    "    days.append(hours)\n",
    "print(days)\n",
    "\n",
    "# i surrender :(\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.For any input date(YYYY-MM-DD format), print week starting and weekend date, consider week starting as monday and week ending as sunday\n",
    "hint : this can be achieved by used python sys modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today: 2020-07-23 00:00:00\n",
      "Start: 2020-07-20 00:00:00\n",
      "End: 2020-07-26 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "input_date = \"2020-07-23\" #change here\n",
    "today = datetime.strptime(input_date, '%Y-%m-%d')\n",
    "start = today - timedelta(days=today.weekday())\n",
    "end = start + timedelta(days=6)\n",
    "print(\"Today: \" + str(today))\n",
    "print(\"Start: \" + str(start))\n",
    "print(\"End: \" + str(end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Example usage of kwargs in python methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of name is Helsa\n",
      "The value of employee_number is 776829902\n",
      "The value of occupation is Big Data Engineer\n"
     ]
    }
   ],
   "source": [
    "def print_values(**kwargs):\n",
    "    for key, value in kwargs.items():\n",
    "        print(\"The value of {} is {}\".format(key, value))\n",
    "\n",
    "print_values(\n",
    "            name=\"Helsa\",\n",
    "            employee_number=\"776829902\",\n",
    "            occupation=\"Big Data Engineer\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.Split a json tree having multiple table structures using python\n",
    "\n",
    "ps  : do not use any third party packages or hardcoded values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('k1', 'v1'), ('k2', 'blank'), ('k3', 'v3'), ('k4', \"'blank\"), ('k5', \"'blank\")])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "str_input = '''{\"k1\":\"v1\", \"k2\":\"blank\",\"k3\":\"v3\",\"k4\":\"'blank\",\"k5\":\"'blank\"}'''\n",
    "json_input = json.loads(str_input)\n",
    "schema_input = json_input.items()\n",
    "print(schema_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i think that's all , thank you and sorry for the inconvenience :)\n",
    "\n",
    "Regards,\n",
    "Helsa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
